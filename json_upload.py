
import psycopg2
import json
import os
import pandas as pd
import traceback
from psycopg2 import sql
import re
import json
import traceback
from tqdm import tqdm # For progress bars during batch operations
from psycopg2.extras import execute_values

def sanitize_column_name(name):
    """Sanitizes column names to be SQL-friendly."""
    if isinstance(name, str):
        # Remove non-alphanumeric (except underscore), replace spaces with underscore, convert to lowercase
        return re.sub(r'\W+', '_', name).lower()
    return name

def determine_sql_data_type(series):
    """Determines the appropriate PostgreSQL data type for a pandas Series."""
    if pd.api.types.is_integer_dtype(series):
        # Use BIGINT to prevent overflow with large IDs/integers
        return 'BIGINT'
    elif pd.api.types.is_float_dtype(series):
        return 'DOUBLE PRECISION'
    elif pd.api.types.is_bool_dtype(series):
        return 'BOOLEAN'
    elif pd.api.types.is_datetime64_any_dtype(series):
        return 'TIMESTAMP WITHOUT TIME ZONE' # Standard for datetime objects

    elif pd.api.types.is_string_dtype(series) or series.dtype == 'object':
        # Attempt to detect strict YYYY-MM-DD dates in string columns
        date_pattern_strict = r"^\d{4}-\d{2}-\d{2}$"
        if series.astype(str).str.match(date_pattern_strict).all() and not series.isnull().all():
            return 'DATE'
        # For all other strings, use TEXT for flexible length
        return 'TEXT'
    else:
        # Default for unknown types or mixed types that Pandas classifies as 'object'
        return 'TEXT'

import numpy as np # Make sure numpy is imported

def convert_to_correct_type(value, expected_type):
    """
    Converts a value to the specified Python type matching the SQL type for psycopg2,
    explicitly handling NumPy types.
    """
    if pd.isna(value):
        return None # Convert pandas NA to None for PostgreSQL NULL

    # --- Add explicit handling for NumPy types first ---
    if isinstance(value, np.integer):
        return int(value)
    elif isinstance(value, np.floating):
        return float(value)
    elif isinstance(value, np.bool_):
        return bool(value)
    # --- End NumPy handling ---

    try:
        if expected_type == 'BIGINT':
            # After NumPy handling, this should primarily catch standard Python ints
            return int(value)
        elif expected_type == 'DOUBLE PRECISION':
            return float(value)
        elif expected_type == 'BOOLEAN':
            return bool(value)
        elif expected_type == 'TIMESTAMP WITHOUT TIME ZONE' or expected_type == 'DATE':
            if hasattr(value, 'to_pydatetime'):
                return value.to_pydatetime()
            # Attempt to convert other date-like objects/strings
            return pd.to_datetime(value, errors='coerce').to_pydatetime() if pd.notna(pd.to_datetime(value, errors='coerce')) else None
        elif expected_type == 'TEXT' or expected_type.startswith('VARCHAR'):
            return str(value)
        else:
            return value # Return as-is for unhandled types (e.g., standard Python types not needing special conversion)
    except (ValueError, TypeError):
        return None # Return None if conversion fails

def validate_table_structure(cur, table_name):
    """Checks if a table exists in the public schema of the database."""
    try:
        cur.execute(f"SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_schema = 'public' AND table_name = '{table_name}')")
        return cur.fetchone()[0]
    except psycopg2.Error as e:
        print(f"Error checking table existence for '{table_name}': {e}")
        return False

def identify_primary_key(df):
    """Identifies a suitable primary key column in a DataFrame."""
    for col in df.columns:
        # Check for non-null values and uniqueness
        if df[col].notna().all() and df[col].duplicated().sum() == 0:
            # Prefer numeric or string types for PKs
            if pd.api.types.is_numeric_dtype(df[col]) or pd.api.types.is_string_dtype(df[col]):
                return col
    return None

def update_table_structure_for_json(cur, conn, table_name, df, db_primary_key_column):
    """
    Updates the structure of an existing table based on the DataFrame's columns.
    Handles adding new columns, dropping missing columns, and altering column types.
    """
    print(f"Updating structure for existing table '{table_name}'.")

    # Get existing columns from DB
    cur.execute(f"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '{table_name}' ORDER BY ordinal_position;")
    existing_columns_info = cur.fetchall()
    existing_columns_names = {row[0] for row in existing_columns_info}
    existing_columns_types = {row[0]: row[1] for row in existing_columns_info}

    # 1. Identify and drop columns that exist in DB but not in new DF (unless it's the primary key)
    columns_to_drop = [
        col for col in existing_columns_names
        if col not in df.columns and col != db_primary_key_column
    ]
    for col in columns_to_drop:
        print(f"Dropping column '{col}' from table '{table_name}'. It's not in the new data.")
        try:
            cur.execute(sql.SQL('ALTER TABLE {} DROP COLUMN {} CASCADE').format( # CASCADE to drop dependent objects
                sql.Identifier(table_name),
                sql.Identifier(col)
            ))
            conn.commit()
        except Exception as e:
            print(f"Error dropping column '{col}': {e}")
            conn.rollback() # Rollback on DDL error

    # 2. Identify and add/alter columns present in new DF
    for col in df.columns:
        target_col_type = determine_sql_data_type(df[col])
        if col not in existing_columns_names:
            # Column doesn't exist, add it
            print(f"Adding new column '{col}' with type '{target_col_type}' to table '{table_name}'.")
            try:
                cur.execute(sql.SQL('ALTER TABLE {} ADD COLUMN {} {}').format(
                    sql.Identifier(table_name),
                    sql.Identifier(col),
                    sql.SQL(target_col_type)
                ))
                conn.commit()
            except Exception as e:
                print(f"Error adding column '{col}': {e}")
                conn.rollback()
        else:
            # Column exists, check if type needs altering (e.g., VARCHAR to TEXT)
            existing_db_type = existing_columns_types.get(col)
            if existing_db_type and existing_db_type != target_col_type:
                # Check for specific upgrades (e.g., VARCHAR(N) to TEXT or wider VARCHAR)
                is_type_upgrade = False
                if existing_db_type.startswith('character varying') and target_col_type == 'TEXT':
                    is_type_upgrade = True
                elif existing_db_type.startswith('character varying') and target_col_type.startswith('VARCHAR'):
                    # Compare lengths for VARCHAR to VARCHAR upgrade
                    try:
                        existing_len = int(re.search(r'\d+', existing_db_type).group())
                        target_len = int(re.search(r'\d+', target_col_type).group())
                        if target_len > existing_len:
                            is_type_upgrade = True
                    except (AttributeError, ValueError): # No length found, or invalid length
                        pass # Cannot compare, skip or consider as mismatch

                if is_type_upgrade:
                    print(f"Altering column '{col}' type from '{existing_db_type}' to '{target_col_type}'.")
                    try:
                        # Use USING clause to safely cast existing data if possible
                        alter_type_query = sql.SQL('ALTER TABLE {} ALTER COLUMN {} TYPE {} USING {}::{}').format(
                            sql.Identifier(table_name),
                            sql.Identifier(col),
                            sql.SQL(target_col_type),
                            sql.Identifier(col),
                            sql.SQL(target_col_type)
                        )
                        cur.execute(alter_type_query)
                        conn.commit()
                    except Exception as e:
                        print(f"Error altering column '{col}' type from '{existing_db_type}' to '{target_col_type}': {e}")
                        conn.rollback()
                # else: print(f"Column '{col}' already has matching type '{existing_db_type}'.")


    # 3. Ensure primary key constraint is correct and applied
    if db_primary_key_column:
        cur.execute(f"""
            SELECT conname FROM pg_constraint pc
            JOIN pg_class pt ON pt.oid = pc.conrelid
            WHERE pt.relname = %s AND contype = 'p';
        """, (table_name,))
        pk_constraint_info = cur.fetchone()

        if pk_constraint_info:
            print(f"Table '{table_name}' already has a primary key constraint '{pk_constraint_info[0]}'.")
            # You might want to check if it's on the correct column and alter if not.
            # This is more complex and often involves dropping and re-adding the constraint.
        else:
            print(f"Adding PRIMARY KEY constraint on column '{db_primary_key_column}' to table '{table_name}'.")
            try:
                cur.execute(sql.SQL('ALTER TABLE {} ADD PRIMARY KEY ({})').format(
                    sql.Identifier(table_name),
                    sql.Identifier(db_primary_key_column)
                ))
                conn.commit()
            except Exception as e:
                print(f"Error adding primary key constraint: {e}")
                conn.rollback()

def flatten_json_data(data):
    """
    Flattens a JSON array of objects with nested structures.
    Converts nested arrays into rows while preserving parent fields.
    """
    def flatten_record(record, parent_key='', parent_row={}):
        """
        Recursively flattens a single JSON object into a row(s). Handles nested objects and lists.
        """
        rows = []
        current_row = parent_row.copy() # Start with inherited parent fields

        # Collect scalar values and process nested structures
        nested_lists_to_process = []
        for key, value in record.items():
            new_key = f"{parent_key}_{key}" if parent_key else key
            if isinstance(value, dict):
                # Recursively flatten dictionaries, passing current_row to maintain context
                rows.extend(flatten_record(value, new_key, current_row))
                current_row = {} # Reset current_row for this branch if it was extended
            elif isinstance(value, list):
                # Defer list processing to ensure all current_row scalars are gathered
                nested_lists_to_process.append((new_key, value))
            else:
                # For scalar values, add to the current row
                current_row[new_key] = value

        # If there were no nested structures or lists, just return the current_row
        if not nested_lists_to_process and not rows:
            rows.append(current_row)

        # Process deferred nested lists
        if nested_lists_to_process:
            if not rows: # If no rows from nested dicts, start with current_row for list expansion
                rows.append(current_row)
            
            new_expanded_rows = []
            for existing_row in rows: # Expand each previously generated row with list content
                for list_key, list_value in nested_lists_to_process:
                    for item in list_value:
                        row_copy = existing_row.copy()
                        if isinstance(item, dict):
                            new_expanded_rows.extend(flatten_record(item, list_key, row_copy))
                        else:
                            row_copy[list_key] = item
                            new_expanded_rows.append(row_copy)
            rows = new_expanded_rows if new_expanded_rows else rows # Use expanded rows if any, else keep original

        return rows if rows else [current_row] # Ensure at least one row is returned


    flattened_rows = []
    for record in data:
        flattened_rows.extend(flatten_record(record))

    # Convert to pandas DataFrame, filling missing keys with NaN
    df = pd.DataFrame(flattened_rows)
    return df


def insert_or_update_batch(cur, conn, table_name, df, primary_key_column, batch_size=5000):
    """
    Performs batch UPSERT (INSERT ... ON CONFLICT DO UPDATE) or batch INSERT
    into the specified table.
    """
    if df.empty:
        print(f"No data to insert into '{table_name}' in this batch.")
        return

    # Get actual columns from the database and identify SERIAL columns
    cur.execute(f"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '{table_name}' ORDER BY ordinal_position;")
    db_cols_info = cur.fetchall()
    db_columns = [c_info[0] for c_info in db_cols_info]
    serial_cols_in_db = [c_info[0] for c_info in db_cols_info if 'nextval' in c_info[1] or 'serial' in c_info[1].lower() or 'identity' in c_info[1].lower()]

    # Columns from DataFrame that will be explicitly inserted/updated.
    # Exclude DB-managed SERIAL/IDENTITY PKs from the list of columns to provide values for.
    cols_to_insert = [col for col in db_columns if col in df.columns and col not in serial_cols_in_db]

    # Prepare values for batch insertion/update
    rows_to_process = []
    for _, row in df.iterrows():
        processed_row = []
        for col in cols_to_insert:
            # Ensure value is converted to a type psycopg2 can handle
            expected_db_type = next((info[1] for info in db_cols_info if info[0] == col), 'TEXT') # Get actual DB type for conversion
            processed_row.append(convert_to_correct_type(row.get(col), expected_db_type))
        rows_to_process.append(tuple(processed_row))

    if primary_key_column and primary_key_column in db_columns: # Ensure PK is actually in DB schema
        # UPSERT scenario:
        # Construct the ON CONFLICT DO UPDATE SET clause
        update_columns = [col for col in cols_to_insert if col != primary_key_column]
        update_set_clause = sql.SQL(', ').join(
            sql.SQL("{} = EXCLUDED.{}").format(sql.Identifier(col), sql.Identifier(col))
            for col in update_columns
        )

        upsert_query = sql.SQL("""
            INSERT INTO {} ({}) VALUES %s
            ON CONFLICT ({}) DO UPDATE SET {}
        """).format(
            sql.Identifier(table_name),
            sql.SQL(', ').join(map(sql.Identifier, cols_to_insert)),
            sql.Identifier(primary_key_column),
            update_set_clause
        )
        print(f"Using UPSERT strategy for table '{table_name}' on primary key '{primary_key_column}'.")
        query_str = upsert_query.as_string(conn)
    else:
        # Simple INSERT scenario (e.g., for tables with SERIAL PK where no explicit PK is provided from data)
        print(f"Using simple INSERT strategy for table '{table_name}'. No UPSERT possible without a conflict target.")
        insert_query = sql.SQL("INSERT INTO {} ({}) VALUES %s").format(
            sql.Identifier(table_name),
            sql.SQL(', ').join(map(sql.Identifier, cols_to_insert))
        )
        query_str = insert_query.as_string(conn)

    try:
        with tqdm(total=len(rows_to_process), desc=f"Inserting into {table_name}", unit="row") as pbar:
            for i in range(0, len(rows_to_process), batch_size):
                batch = rows_to_process[i:i + batch_size]
                execute_values(
                    cur,
                    query_str,
                    batch,
                    page_size=batch_size
                )
                conn.commit() # Commit each batch
                pbar.update(len(batch))

        print(f"Processed {len(rows_to_process)} rows in table '{table_name}'.")
    except Exception as e:
        print(f"Error during batch operation: {str(e)}")
        traceback.print_exc()
        conn.rollback() # Rollback the current transaction on error
        raise e



def insert_or_update_batch(cur, conn, table_name, df, primary_key_column, batch_size=5000):
    """
    Performs batch UPSERT (INSERT ... ON CONFLICT DO UPDATE) or batch INSERT
    into the specified table.
    """
    if df.empty:
        print(f"No data to insert into '{table_name}' in this batch.")
        return

    # Get actual columns from the database and identify SERIAL columns
    cur.execute(f"SELECT column_name, data_type FROM information_schema.columns WHERE table_name = '{table_name}' ORDER BY ordinal_position;")
    db_cols_info = cur.fetchall()
    db_columns = [c_info[0] for c_info in db_cols_info]
    serial_cols_in_db = [c_info[0] for c_info in db_cols_info if 'nextval' in c_info[1] or 'serial' in c_info[1].lower() or 'identity' in c_info[1].lower()]

    # Columns from DataFrame that will be explicitly inserted/updated.
    # Exclude DB-managed SERIAL/IDENTITY PKs from the list of columns to provide values for.
    cols_to_insert = [col for col in db_columns if col in df.columns and col not in serial_cols_in_db]

    # Prepare values for batch insertion/update
    rows_to_process = []
    for _, row in df.iterrows():
        processed_row = []
        for col in cols_to_insert:
            # Ensure value is converted to a type psycopg2 can handle
            expected_db_type = next((info[1] for info in db_cols_info if info[0] == col), 'TEXT') # Get actual DB type for conversion
            processed_row.append(convert_to_correct_type(row.get(col), expected_db_type))
        rows_to_process.append(tuple(processed_row))

    if primary_key_column and primary_key_column in db_columns: # Ensure PK is actually in DB schema
        # UPSERT scenario:
        # Construct the ON CONFLICT DO UPDATE SET clause
        update_columns = [col for col in cols_to_insert if col != primary_key_column]
        update_set_clause = sql.SQL(', ').join(
            sql.SQL("{} = EXCLUDED.{}").format(sql.Identifier(col), sql.Identifier(col))
            for col in update_columns
        )

        upsert_query = sql.SQL("""
            INSERT INTO {} ({}) VALUES %s
            ON CONFLICT ({}) DO UPDATE SET {}
        """).format(
            sql.Identifier(table_name),
            sql.SQL(', ').join(map(sql.Identifier, cols_to_insert)),
            sql.Identifier(primary_key_column),
            update_set_clause
        )
        print(f"Using UPSERT strategy for table '{table_name}' on primary key '{primary_key_column}'.")
        query_str = upsert_query.as_string(conn)
    else:
        # Simple INSERT scenario (e.g., for tables with SERIAL PK where no explicit PK is provided from data)
        print(f"Using simple INSERT strategy for table '{table_name}'. No UPSERT possible without a conflict target.")
        insert_query = sql.SQL("INSERT INTO {} ({}) VALUES %s").format(
            sql.Identifier(table_name),
            sql.SQL(', ').join(map(sql.Identifier, cols_to_insert))
        )
        query_str = insert_query.as_string(conn)

    try:
        with tqdm(total=len(rows_to_process), desc=f"Inserting into {table_name}", unit="row") as pbar:
            for i in range(0, len(rows_to_process), batch_size):
                batch = rows_to_process[i:i + batch_size]
                execute_values(
                    cur,
                    query_str,
                    batch,
                    page_size=batch_size
                )
                conn.commit() # Commit each batch
                pbar.update(len(batch))

        print(f"Processed {len(rows_to_process)} rows in table '{table_name}'.")
    except Exception as e:
        print(f"Error during batch operation: {str(e)}")
        traceback.print_exc()
        conn.rollback() # Rollback the current transaction on error
        raise e

# --- Main Function (Updated) ---

def upload_json_to_postgresql(database_name, username, password, json_file_path, user_provided_primary_key=None, host='localhost', port='5432'):
    conn = None
    cur = None
    try:
        conn = psycopg2.connect(
            dbname=database_name,
            user=username,
            password=password,
            host=host,
            port=port,
            connect_timeout=30
        )
        cur = conn.cursor()
        # Set session parameters for performance
        cur.execute("SET synchronous_commit = OFF")
        cur.execute("SET commit_delay = 0")
        cur.execute("SET work_mem = '256MB'")
        cur.execute("SET maintenance_work_mem = '512MB'")

        with open(json_file_path, 'r') as f:
            data = json.load(f)

        json_array = None
        if isinstance(data, list):
            json_array = data
        elif isinstance(data, dict):
            # If it's a dict with a single key pointing to a list or another dict, treat that as the main data
            if len(data) == 1:
                potential_array_or_object = list(data.values())[0]
                if isinstance(potential_array_or_object, list):
                    json_array = potential_array_or_object
                elif isinstance(potential_array_or_object, dict):
                    json_array = [potential_array_or_object] # Wrap single dict in a list
            else:
                raise ValueError("If JSON root is an object, it must contain a single key pointing to the array/object data.")
        else:
            raise ValueError("Invalid JSON format. The root element must be an array or an object containing an array/object.")

        df = flatten_json_data(json_array)

        # Sanitize DataFrame column names
        df.columns = [sanitize_column_name(col) for col in df.columns]

        # Determine the table name from the JSON file name
        table_name = os.path.splitext(os.path.basename(json_file_path))[0]
        table_name = sanitize_column_name(table_name)

        # --- Primary Key Identification Logic ---
        db_primary_key_column = None
        is_serial_pk = False # Flag to indicate if the PK is a DB-managed SERIAL/IDENTITY

        # 1. Try user-provided primary key
        if user_provided_primary_key:
            sanitized_user_pk = sanitize_column_name(user_provided_primary_key)
            if sanitized_user_pk in df.columns and df[sanitized_user_pk].notna().all() and df[sanitized_user_pk].duplicated().sum() == 0:
                db_primary_key_column = sanitized_user_pk
                print(f"Using user-provided primary key: '{db_primary_key_column}'.")
            else:
                print(f"Warning: User-provided primary key '{user_provided_primary_key}' is invalid (not found, has nulls, or duplicates).")

        # 2. If no valid user-provided PK, try to identify from data
        if db_primary_key_column is None:
            identified_pk = identify_primary_key(df)
            if identified_pk:
                db_primary_key_column = identified_pk
                print(f"Identified primary key from data: '{db_primary_key_column}'.")
            else:
                # 3. Fallback to DB-managed 'id' (SERIAL/IDENTITY)
                db_primary_key_column = 'id'
                is_serial_pk = True
                print("No natural primary key found. Database will manage 'id' as SERIAL primary key.")

        # --- Table Structure Validation and Update ---
        table_exists = validate_table_structure(cur, table_name)

        if not table_exists:
            print(f"Creating table '{table_name}'.")
            column_defs = []
            for col in df.columns:
                col_type = determine_sql_data_type(df[col])
                column_defs.append(f'"{col}" {col_type}')

            # Add primary key definition to CREATE TABLE statement
            if is_serial_pk:
                # Add 'id' as GENERATED BY DEFAULT AS IDENTITY for auto-incrementing
                # Ensure it's not already in column_defs if it exists in DF for some reason
                if db_primary_key_column not in [col_def.split(' ')[0].strip('"') for col_def in column_defs]:
                    column_defs.insert(0, f'"{db_primary_key_column}" BIGINT PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY')
            elif db_primary_key_column and db_primary_key_column in df.columns:
                # If it's a natural PK from the data, append PRIMARY KEY to its definition
                for i, col_def in enumerate(column_defs):
                    if col_def.startswith(f'"{db_primary_key_column}"'):
                        column_defs[i] += ' PRIMARY KEY'
                        break
                else: # Should not happen if db_primary_key_column is from df.columns
                    print(f"Warning: Primary key column '{db_primary_key_column}' not found in initial column definitions for PK application.")

            create_table_query = sql.SQL('CREATE TABLE {} ({})').format(
                sql.Identifier(table_name),
                sql.SQL(', ').join(sql.SQL(col_def) for col_def in column_defs)
            )
            try:
                print(f"Generated CREATE TABLE Query: {create_table_query.as_string(conn)}")
                cur.execute(create_table_query)
                conn.commit()
                if not validate_table_structure(cur, table_name):
                    raise Exception(f"Table '{table_name}' was not created successfully.")
            except Exception as e:
                print(f"Error creating table {table_name}: {str(e)}")
                traceback.print_exc()
                conn.rollback()
                return f"Error: Could not create table {table_name}. {str(e)}"
        else:
            print(f"Table '{table_name}' already exists. Updating its structure if necessary.")
            # Update structure (add/drop columns, alter types, ensure PK constraint)
            update_table_structure_for_json(cur, conn, table_name, df, db_primary_key_column)


        # --- Data Insertion/Update (Batch UPSERT/INSERT) ---
        print(f"Starting data insertion into table '{table_name}'.")

        # If it's a serial PK and we are re-uploading a file that should completely replace data,
        # TRUNCATE might be desired before insert_or_update_batch.
        # For now, `insert_or_update_batch` will either UPSERT (if a natural PK exists in DF)
        # or simply INSERT (if PK is SERIAL and not provided in DF).
        insert_or_update_batch(cur, conn, table_name, df, db_primary_key_column, batch_size=5000)

        # --- Save details in the datasource table ---
        json_save_path = os.path.join('uploads', f"{table_name}.json")
        os.makedirs(os.path.dirname(json_save_path), exist_ok=True)
        # Save the *processed* DataFrame to JSON
        df.to_json(json_save_path, orient='records', indent=4)
        print(f"Processed JSON saved to: {json_save_path}")

        # Create a datasource table if it doesn't exist
        cur.execute("""
            CREATE TABLE IF NOT EXISTS datasource (
                id SERIAL PRIMARY KEY,
                data_source_name VARCHAR(255) UNIQUE, -- Added UNIQUE constraint
                data_source_path VARCHAR(255)
            );
        """)
        conn.commit() # Commit DDL for datasource table

        # Check if data source entry already exists to avoid duplicates and update path
        cur.execute("SELECT id FROM datasource WHERE data_source_name = %s;", (table_name,))
        existing_data_source_entry = cur.fetchone()

        if existing_data_source_entry:
            print(f"Data source entry for '{table_name}' already exists. Updating path.")
            cur.execute("""
                UPDATE datasource SET data_source_path = %s WHERE data_source_name = %s;
            """, (json_save_path, table_name))
        else:
            print(f"Adding new data source entry for '{table_name}'.")
            cur.execute("""
                INSERT INTO datasource (data_source_name, data_source_path)
                VALUES (%s, %s);
            """, (table_name, json_save_path))

        conn.commit() # Commit DML for datasource table

        return "Upload successful"
    except Exception as e:
        print("An error occurred:", e)
        traceback.print_exc()
        if conn:
            conn.rollback() # Ensure rollback on any unhandled exception
        return f"Error: {str(e)}"
    finally:
        # Always reset session parameters and close connection
        if cur:
            try:
                cur.execute("RESET synchronous_commit")
                cur.execute("RESET commit_delay")
                cur.execute("RESET work_mem")
                cur.execute("RESET maintenance_work_mem")
            except Exception as e:
                print(f"Error resetting session parameters: {e}")
            finally:
                cur.close()
        if conn:
            conn.close()
            print("Database connection closed.")